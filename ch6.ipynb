{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import common.time_layers as TL\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, H, D = vocab_size, wordvec_size, hidden_size\n",
    "        normal = np.random.normal\n",
    "        mu = 0\n",
    "        root = np.sqrt\n",
    "        embed_W = normal(mu, 0.01, (V, D))\n",
    "        lstm_Wx = normal(mu, 1/root(D), (D, 4*H))\n",
    "        lstm_Wh = normal(mu, 1/root(H), (H, 4*H))\n",
    "        lstm_b = np.zeros(4*H, dtype=np.float_)\n",
    "        affine_W = normal(mu, 1/root(H), (H, V))\n",
    "        affine_b = np.zeros(V, dtype=np.float_)\n",
    "\n",
    "        self.layers = (\n",
    "            TL.TimeEmbedding(embed_W),\n",
    "            TL.TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TL.TimeAffine(embed_W.T, affine_b)\n",
    "        )\n",
    "        self.loss_layer = TL.TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params.extend(layer.params)\n",
    "            self.grads.extend(layer.grads)\n",
    "\n",
    "    def predict(self, xs: np.ndarray):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "\n",
    "    def save_params(self, fname=\"Rnnlm.pkl\"):\n",
    "        with open(fname, \"wb\")as f:\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    def load_params(self, fname=\"Rnnlm.pkl\"):\n",
    "        with open(fname, \"rb\") as f:\n",
    "            self.params = pickle.load(f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# coding: utf-8\n",
    "from common.optimizers import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNNの隠れ状態ベクトルの要素数\n",
    "time_size = 35  # RNNを展開するサイズ\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 学習データの読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# モデルの生成\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 勾配クリッピングを適用して学習\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# テストデータで評価\n",
    "model.rest_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('test perplexity: ', ppl_test)\n",
    "\n",
    "# パラメータの保存\n",
    "model.save_params()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "evaluating perplexity ...\n",
      "234 / 235\n",
      "test perplexity:  134.51278397479436\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from common.functions import softmax\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id: int, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "        x = start_id\n",
    "\n",
    "        while (len(word_ids) < sample_size):\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())\n",
    "            sampleId = np.random.choice(len(p), size=1, p=p)\n",
    "\n",
    "            if (skip_ids is None) or (sampleId not in skip_ids):\n",
    "                x = sampleId\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from dataset import ptb\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('./Rnnlm.pkl')\n",
    "\n",
    "# start文字とskip文字の設定\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "# 文章生成\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "you allows buckle pursue christies adopting aerospace pinpoint eventual equivalents audits scams cray-3 shop reducing deprived carol or 's efficient respectability arms-control trudeau purpose agrees bringing regulate certificates buy dover boring basir hitachi ordinary leval rep fares teller packages balance advise charter highway beef consolidating ordinarily conner malignant subscribe selected einhorn lease capped opening latter dislike competitive guerrilla fashion hire contras erased creation arrangement running strict turns living wonderful impressive snapped ease covered averaging impose technical brushed instead french reasonable viewpoint b pont freeways trans anticipation tours compelling guaranty grant shuttle manufacturers shed cameras takeovers canceled relief rica sick shamir\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('.venv': pipenv)"
  },
  "interpreter": {
   "hash": "64427789a60298f8e18e8f8b0964a7b8dbbf9caad5f30543b6bb511c9aa0d75f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}